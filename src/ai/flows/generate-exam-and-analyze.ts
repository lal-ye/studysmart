// This is an autogenerated file from Firebase Studio.
'use server';
/**
 * @fileOverview Generates an exam, grades it, and analyzes the results to provide a personalized study plan.
 *
 * - generateExamAndAnalyze - A function that orchestrates the exam generation, grading, and analysis process.
 * - GenerateExamAndAnalyzeInput - The input type for the generateExamAndAnalyze function.
 * - GenerateExamAndAnalyzeOutput - The return type for the generateExamAndAnalyze function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';
import {searchArticles} from '@/services/search-articles';

const ExamQuestionSchema = z.object({
  question: z.string().describe('The exam question.'),
  type: z.enum(['multiple_choice', 'true_false', 'short_answer']).describe('The type of the question.'),
  options: z.array(z.string()).optional().describe('The multiple-choice options. Only present if type is multiple_choice. Should be an array of 4 strings.'),
  correctAnswer: z.string().describe('The correct answer to the question. For true/false, this should be "true" or "false".'),
  topic: z.string().describe('The topic of the question in the course.'),
});
export type ExamQuestion = z.infer<typeof ExamQuestionSchema>;


const GenerateExamAndAnalyzeInputSchema = z.object({
  courseMaterial: z
    .string()
    .describe('The course material in PDF or text format.'),
  numberOfQuestions: z
    .number()
    .default(30) 
    .describe('The desired number of questions in the exam. Should be 30 for the standard exam format.'),
  userAnswers: z.array(z.string()).optional().describe('Optional user answers for grading. If not provided, only exam generation occurs. The order must match the exam questions.'),
  exam: z.array(ExamQuestionSchema).optional().describe('The original exam questions. Required if userAnswers are provided for grading, otherwise questions will be generated.')
});

export type GenerateExamAndAnalyzeInput = z.infer<
  typeof GenerateExamAndAnalyzeInputSchema
>;


const ExamResultSchema = z.object({
  question: z.string().describe('The exam question.'),
  type: z.enum(['multiple_choice', 'true_false', 'short_answer']).describe('The type of the question.'),
  correctAnswer: z.string().describe('The correct answer to the question.'),
  userAnswer: z.string().describe('The user provided answer.'),
  isCorrect: z.boolean().describe('Whether the user answer is correct.'),
  topic: z.string().describe('The topic of the question in the course.'),
});
export type ExamResult = z.infer<typeof ExamResultSchema>;

const GenerateExamAndAnalyzeOutputSchema = z.object({
  exam: z.array(ExamQuestionSchema).describe('The generated exam (or the exam that was graded).'),
  results: z.array(ExamResultSchema).describe('The graded exam results. Empty if only generation was performed.'),
  topicsToReview: z
    .array(z.string())
    .describe('The topics the user needs to review. Empty if only generation was performed.'),
  extraReadings: z
    .array(z.object({title: z.string(), url: z.string()}))
    .describe('Links and articles for extra readings. Empty if only generation was performed.'),
});

export type GenerateExamAndAnalyzeOutput = z.infer<
  typeof GenerateExamAndAnalyzeOutputSchema
>;

export async function generateExamAndAnalyze(
  input: GenerateExamAndAnalyzeInput
): Promise<GenerateExamAndAnalyzeOutput> {
  // Ensure numberOfQuestions is 30 if not specified for this specific flow's logic.
  // This default is mostly for the generation phase.
  const validatedInput = { ...input, numberOfQuestions: input.numberOfQuestions || 30 };
  return generateExamAndAnalyzeFlow(validatedInput);
}

const generateExamPrompt = ai.definePrompt({
  name: 'generateExamPrompt',
  input: {schema: GenerateExamAndAnalyzeInputSchema}, // Will only use courseMaterial and numberOfQuestions from this schema
  output: {schema: z.object({ exam: z.array(ExamQuestionSchema) }) }, 
  prompt: `You are an expert in education and creating effective exams.
Based on the provided course material, generate an exam.
The exam should have exactly {{{numberOfQuestions}}} questions, structured precisely as follows:
- 15 multiple-choice questions. Each must have exactly 4 string options. The 'type' field for these must be 'multiple_choice'.
- 10 true/false questions. The 'type' field for these must be 'true_false'. The 'correctAnswer' must be "true" or "false".
- 5 short answer questions. The 'type' field for these must be 'short_answer'.

For ALL questions, you MUST include:
- 'question': The text of the question (string).
- 'type': One of 'multiple_choice', 'true_false', 'short_answer' (string enum).
- 'options': An array of 4 strings for 'multiple_choice' questions. This field MUST be present and contain 4 strings for multiple_choice type. For 'true_false' and 'short_answer' questions, this field should be omitted or explicitly null.
- 'correctAnswer': The correct answer (string). For true/false, this should be 'true' or 'false'.
- 'topic': The topic from the course material that the question is related to (string).

Output the exam as a SINGLE JSON object containing one key: "exam". The value of "exam" should be an array of question objects matching the schema described.

Course Material:
{{{courseMaterial}}}
`,
});


const gradeExamPrompt = ai.definePrompt({
  name: 'gradeExamPrompt',
  input: {schema: z.object({
    examString: z.string().describe('The exam questions as a JSON string. Each question object includes its type, correctAnswer, and topic.'),
    userAnswersString: z.string().describe('The user answers as a JSON string, in corresponding order to exam questions.'),
  })},
  output: {schema: z.object({ results: z.array(ExamResultSchema) }) }, 
  prompt: `You are an expert in grading exams.
Based on the provided exam questions (including their types, correct answers, and topics) and the user's answers, grade the exam.
The exam questions and user answers are provided as JSON strings.

For each question, determine if the user's answer is correct.
- For 'multiple_choice' and 'true_false' questions, correctness is a direct match with the provided 'correctAnswer'.
- For 'short_answer' questions, evaluate if the user's answer demonstrates understanding equivalent to the 'correctAnswer'. Be lenient with minor phrasing differences if the core concept is correct.

Return a JSON object with a "results" key. The value of "results" should be an array of result objects. Each result object must include:
- 'question': The original question text.
- 'type': The type of the question ('multiple_choice', 'true_false', 'short_answer').
- 'correctAnswer': The correct answer.
- 'userAnswer': The user's provided answer.
- 'isCorrect': A boolean indicating if the user's answer was correct.
- 'topic': The topic of the question.

Exam Questions (JSON string):
{{{examString}}}

User Answers (JSON string, in corresponding order to exam questions):
{{{userAnswersString}}}
`,
});


const analyzeResultsPrompt = ai.definePrompt({
  name: 'analyzeResultsPrompt',
  input: {schema: z.object({
    resultsString: z.string().describe('The exam results as a JSON string. Each result includes the question, user answer, correctness, and topic.'),
  })},
  output: {schema: z.object({ topicsToReview: z.array(z.string()) }) }, 
  prompt: `You are an expert in education and analyzing exam results.
Based on the provided exam results (which include the question, user answer, correctness, and topic for each), identify the topics where the user made mistakes.
The exam results are provided as a JSON string.
Focus on topics associated with incorrectly answered questions.
Return a JSON object with a "topicsToReview" key, containing an array of unique topic strings that the user needs to review.

Exam Results (JSON string):
{{{resultsString}}}
`,
});


const generateExamAndAnalyzeFlow = ai.defineFlow(
  {
    name: 'generateExamAndAnalyzeFlow',
    inputSchema: GenerateExamAndAnalyzeInputSchema,
    outputSchema: GenerateExamAndAnalyzeOutputSchema,
  },
  async (input: GenerateExamAndAnalyzeInput) => {
    let examQuestions: ExamQuestion[];

    if (input.exam && input.userAnswers && input.exam.length > 0) {
      // Grading an existing exam provided in the input
      examQuestions = input.exam;
    } else {
      // Generating a new exam
      const examGenerationResult = await generateExamPrompt({ 
        courseMaterial: input.courseMaterial, 
        numberOfQuestions: input.numberOfQuestions,
        // userAnswers and exam are not relevant for this prompt call
      });
      if (!examGenerationResult.output || !examGenerationResult.output.exam) {
          throw new Error('Failed to generate exam questions from the model.');
      }
      examQuestions = examGenerationResult.output.exam;
    }

    // If only exam generation was requested (no userAnswers), return early
    if (!input.userAnswers || input.userAnswers.length === 0) {
      return {
        exam: examQuestions,
        results: [], 
        topicsToReview: [],
        extraReadings: [],
      };
    }

    // Proceed with grading
    const userAnswersToGrade = input.userAnswers;

    const gradeExamInput = {
      examString: JSON.stringify(examQuestions),
      userAnswersString: JSON.stringify(userAnswersToGrade),
    };

    const llmGradingResult = await gradeExamPrompt(gradeExamInput);
    if (!llmGradingResult.output || !llmGradingResult.output.results) {
        throw new Error('Failed to get initial grading results from the model.');
    }
    const preliminaryResults = llmGradingResult.output.results;

    // Refine grading: Deterministic for MC/TF, use LLM's grade for Short Answer
    const finalExamResults: ExamResult[] = examQuestions.map((question, index) => {
      const userAnswer = userAnswersToGrade[index] || ""; // Ensure userAnswer is always a string
      // Find the corresponding preliminary result from the LLM
      // This assumes the LLM returns results in the same order or includes enough info to match.
      // A more robust match would be on question text if order isn't guaranteed.
      // For now, we assume order or rely on the LLM to match questions if it reorders.
      const llmGradedResult = preliminaryResults.find(r => r.question === question.question && r.topic === question.topic);


      let isCorrectDetermined: boolean;
      if (question.type === 'multiple_choice' || question.type === 'true_false') {
        isCorrectDetermined = userAnswer === question.correctAnswer;
      } else { // short_answer
        // For short answers, trust the LLM's judgment on correctness.
        isCorrectDetermined = llmGradedResult ? llmGradedResult.isCorrect : false; 
      }
      
      return {
        question: question.question,
        type: question.type,
        correctAnswer: question.correctAnswer,
        userAnswer: userAnswer,
        isCorrect: isCorrectDetermined,
        topic: question.topic,
      };
    });

    const analyzeResultsInput = {
      resultsString: JSON.stringify(finalExamResults), 
    };

    const analysisResult = await analyzeResultsPrompt(analyzeResultsInput);
    if (!analysisResult.output || !analysisResult.output.topicsToReview) {
        throw new Error('Failed to analyze exam results using the model.');
    }
    const topicsToReview = analysisResult.output.topicsToReview;

    const extraReadingsPromises = topicsToReview.map(topic => searchArticles(topic));
    const extraReadingsArrays = await Promise.all(extraReadingsPromises);
    const extraReadings = extraReadingsArrays.flat().filter(article => article.title !== "No Relevant Articles Found");


    return {
      exam: examQuestions,
      results: finalExamResults,
      topicsToReview: topicsToReview,
      extraReadings: extraReadings,
    };
  }
);